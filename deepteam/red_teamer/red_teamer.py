import asyncio
from tqdm import tqdm
from typing import Dict, List, Optional, Union
from rich.console import Console
from rich.table import Table
import inspect
from rich import box


from deepeval.models import DeepEvalBaseLLM
from deepeval.metrics.utils import initialize_model
from deepeval.dataset.golden import Golden
from deepeval.test_case import LLMTestCase
from deepeval.utils import get_or_create_event_loop

from deepteam.frameworks.frameworks import AISafetyFramework
from deepteam.telemetry import capture_red_teamer_run
from deepteam.attacks import BaseAttack
from deepteam.vulnerabilities import BaseVulnerability
from deepteam.vulnerabilities.custom.custom import CustomVulnerability
from deepteam.vulnerabilities.types import (
    IntellectualPropertyType,
    IllegalActivityType,
    PersonalSafetyType,
    GraphicContentType,
    MisinformationType,
    PromptLeakageType,
    CompetitionType,
    PIILeakageType,
    ToxicityType,
    BiasType,
    RBACType,
    BOLAType,
    BFLAType,
    SSRFType,
    DebugAccessType,
    ShellInjectionType,
    SQLInjectionType,
    VulnerabilityType,
)

# Import agentic vulnerability types
from deepteam.vulnerabilities.agentic.recursive_hijacking.types import (
    RecursiveHijackingType,
)
from deepteam.vulnerabilities.agentic.goal_theft.types import GoalTheftType
from deepteam.vulnerabilities.agentic.robustness.types import RobustnessType
from deepteam.vulnerabilities.agentic.excessive_agency.types import (
    ExcessiveAgencyType,
)

from deepteam.attacks.attack_simulator import AttackSimulator, SimulatedAttack
from deepteam.attacks.multi_turn.types import CallbackType
from deepteam.metrics import (
    BaseRedTeamingMetric,
    BiasMetric,
    HarmMetric,
    PromptExtractionMetric,
    PIIMetric,
    RBACMetric,
    DebugAccessMetric,
    ShellInjectionMetric,
    SQLInjectionMetric,
    BFLAMetric,
    BOLAMetric,
    SSRFMetric,
    ExcessiveAgencyMetric,
    HijackingMetric,
    IntellectualPropertyMetric,
    OverrelianceMetric,
    CompetitorsMetric,
    ToxicityMetric,
    MisinformationMetric,
    GraphicMetric,
    SafetyMetric,
    IllegalMetric,
)
from deepteam.metrics.toxicity.toxicity import ToxicityMetric
from deepteam.metrics.misinformation.misinformation import MisinformationMetric
from deepteam.metrics.illegal_activity.illegal_activity import IllegalMetric
from deepteam.metrics.graphic_content.graphic_content import GraphicMetric
from deepteam.metrics.personal_safety.personal_safety import SafetyMetric

# Import agentic metrics
from deepteam.metrics.agentic.subversion_success.subversion_success import (
    SubversionSuccessMetric,
)
from deepteam.metrics.agentic.extraction_success.extraction_success import (
    ExtractionSuccessMetric,
)
from deepteam.metrics.agentic.verification_assessment.verification_assessment import (
    VerificationAssessmentMetric,
)


from deepteam.red_teamer.utils import group_attacks_by_vulnerability_type
from deepteam.red_teamer.risk_assessment import (
    construct_risk_assessment_overview,
    RedTeamingTestCase,
    RiskAssessment,
)
from deepteam.risks import getRiskCategory


class RedTeamer:
    risk_assessment: Optional[RiskAssessment] = None
    simulated_attacks: Optional[List[SimulatedAttack]] = None

    def __init__(
        self,
        simulator_model: Optional[
            Union[str, DeepEvalBaseLLM]
        ] = "gpt-3.5-turbo-0125",
        evaluation_model: Optional[Union[str, DeepEvalBaseLLM]] = "gpt-4o",
        target_purpose: Optional[str] = "",
        async_mode: bool = True,
        max_concurrent: int = 10,
    ):
        self.target_purpose = target_purpose
        self.simulator_model, _ = initialize_model(simulator_model)
        self.evaluation_model, _ = initialize_model(evaluation_model)
        self.async_mode = async_mode
        self.synthetic_goldens: List[Golden] = []
        self.max_concurrent = max_concurrent
        self.attack_simulator = AttackSimulator(
            simulator_model=self.simulator_model,
            purpose=self.target_purpose,
            max_concurrent=max_concurrent,
        )

    def red_team(
        self,
        model_callback: CallbackType,
        vulnerabilities: Optional[List[BaseVulnerability]] = None,
        attacks: Optional[List[BaseAttack]] = None,
        framework: Optional[AISafetyFramework] = None,
        attacks_per_vulnerability_type: int = 1,
        ignore_errors: bool = False,
        reuse_simulated_attacks: bool = False,
        metadata: Optional[dict] = None,
    ):
        if framework:
            vulnerabilities = framework.vulnerabilities
            attacks = framework.attacks
        else:
            if not vulnerabilities:
                raise ValueError(
                    "You must either provide a 'framework' or 'vulnerabilities'."
                )

        print(f"Vulnerabilities: {vulnerabilities}")

        if self.async_mode:
            assert inspect.iscoroutinefunction(
                model_callback
            ), "`model_callback` needs to be async. `async_mode` has been set to True."
            loop = get_or_create_event_loop()
            return loop.run_until_complete(
                self.a_red_team(
                    model_callback=model_callback,
                    attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                    vulnerabilities=vulnerabilities,
                    attacks=attacks,
                    ignore_errors=ignore_errors,
                    reuse_simulated_attacks=reuse_simulated_attacks,
                    metadata=metadata,
                )
            )
        else:
            assert not inspect.iscoroutinefunction(
                model_callback
            ), "`model_callback` needs to be sync. `async_mode` has been set to False."
            with capture_red_teamer_run(
                vulnerabilities=[v.get_name() for v in vulnerabilities],
                attacks=[a.get_name() for a in attacks],
            ):
                # Initialize metric map
                metrics_map = self.get_red_teaming_metrics_map(vulnerabilities)
                # Simulate attacks
                if (
                    reuse_simulated_attacks
                    and self.simulated_attacks is not None
                    and len(self.simulated_attacks) > 0
                ):
                    simulated_attacks: List[SimulatedAttack] = (
                        self.simulated_attacks
                    )
                else:
                    self.attack_simulator.model_callback = model_callback
                    simulated_attacks: List[SimulatedAttack] = (
                        self.attack_simulator.simulate(
                            attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                            vulnerabilities=vulnerabilities,
                            attacks=attacks,
                            ignore_errors=ignore_errors,
                            metadata=metadata,
                        )
                    )

                vulnerability_type_to_attacks_map = (
                    group_attacks_by_vulnerability_type(simulated_attacks)
                )
                red_teaming_test_cases: List[RedTeamingTestCase] = []
                total_vulnerability_types = sum(
                    len(v.get_types()) for v in vulnerabilities
                )
                pbar = tqdm(
                    vulnerability_type_to_attacks_map.items(),
                    desc=f"ðŸ“ Evaluating {total_vulnerability_types} vulnerability types across {len(vulnerabilities)} vulnerability(s)",
                )
                for vulnerability_type, simulated_attacks in pbar:
                    for simulated_attack in simulated_attacks:
                        metric: BaseRedTeamingMetric = metrics_map.get(
                            vulnerability_type
                        )()

                        red_teaming_test_case = RedTeamingTestCase(
                            vulnerability=simulated_attack.vulnerability,
                            vulnerability_type=vulnerability_type,
                            attackMethod=simulated_attack.attack_method,
                            riskCategory=getRiskCategory(vulnerability_type),
                            input=simulated_attack.input,
                            metadata=simulated_attack.metadata,
                        )

                        # this will only go through if ignore_errors == True
                        if simulated_attack.error:
                            red_teaming_test_case.error = simulated_attack.error
                            red_teaming_test_cases.append(red_teaming_test_case)
                            continue

                        try:
                            target_output = model_callback(
                                simulated_attack.input
                            )
                            red_teaming_test_case.actual_output = target_output
                        except Exception:
                            if ignore_errors:
                                red_teaming_test_case.error = (
                                    "Error generating output from target LLM"
                                )
                                red_teaming_test_cases.append(
                                    red_teaming_test_case
                                )
                                continue
                            else:
                                raise

                        test_case = LLMTestCase(
                            input=simulated_attack.input,
                            actual_output=target_output,
                        )
                        try:
                            metric.measure(test_case)
                            red_teaming_test_case.score = metric.score
                            red_teaming_test_case.reason = metric.reason
                        except Exception:
                            if ignore_errors:
                                red_teaming_test_case.error = f"Error evaluating target LLM output for the '{vulnerability_type.value}' vulnerability"
                                red_teaming_test_cases.append(
                                    red_teaming_test_case
                                )
                                continue
                            else:
                                raise

                        red_teaming_test_cases.append(red_teaming_test_case)

                self.risk_assessment = RiskAssessment(
                    overview=construct_risk_assessment_overview(
                        red_teaming_test_cases=red_teaming_test_cases
                    ),
                    test_cases=red_teaming_test_cases,
                )

                self.save_test_cases_as_simulated_attacks(
                    test_cases=red_teaming_test_cases
                )
                self._print_risk_assessment()
                return self.risk_assessment

    async def a_red_team(
        self,
        model_callback: CallbackType,
        vulnerabilities: Optional[List[BaseVulnerability]] = None,
        attacks: Optional[List[BaseAttack]] = None,
        framework: Optional[AISafetyFramework] = None,
        attacks_per_vulnerability_type: int = 1,
        ignore_errors: bool = False,
        reuse_simulated_attacks: bool = False,
        metadata: Optional[dict] = None,
    ):
        if framework:
            vulnerabilities = framework.vulnerabilities
            attacks = framework.attacks
        else:
            if not vulnerabilities:
                raise ValueError(
                    "You must either provide a 'framework' or 'vulnerabilities'."
                )

        with capture_red_teamer_run(
            vulnerabilities=[v.get_name() for v in vulnerabilities],
            attacks=[a.get_name() for a in attacks],
        ):
            # Initialize metric map
            metrics_map = self.get_red_teaming_metrics_map(vulnerabilities)

            # Generate attacks
            if (
                reuse_simulated_attacks
                and self.simulated_attacks is not None
                and len(self.simulated_attacks) > 0
            ):
                simulated_attacks: List[SimulatedAttack] = (
                    self.simulated_attacks
                )
            else:
                self.attack_simulator.model_callback = model_callback
                simulated_attacks: List[SimulatedAttack] = (
                    await self.attack_simulator.a_simulate(
                        attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                        vulnerabilities=vulnerabilities,
                        attacks=attacks,
                        ignore_errors=ignore_errors,
                        metadata=metadata,
                    )
                )
            # Create a mapping of vulnerabilities to attacks
            vulnerability_type_to_attacks_map: Dict[
                VulnerabilityType, List[SimulatedAttack]
            ] = {}
            for simulated_attack in simulated_attacks:
                if (
                    simulated_attack.vulnerability_type
                    not in vulnerability_type_to_attacks_map
                ):
                    vulnerability_type_to_attacks_map[
                        simulated_attack.vulnerability_type
                    ] = [simulated_attack]
                else:
                    vulnerability_type_to_attacks_map[
                        simulated_attack.vulnerability_type
                    ].append(simulated_attack)

            semaphore = asyncio.Semaphore(self.max_concurrent)
            total_attacks = sum(
                len(attacks)
                for attacks in vulnerability_type_to_attacks_map.values()
            )
            num_vulnerability_types = sum(
                len(v.get_types()) for v in vulnerabilities
            )
            pbar = tqdm(
                total=total_attacks,
                desc=f"ðŸ“ Evaluating {num_vulnerability_types} vulnerability types across {len(vulnerabilities)} vulnerability(s)",
            )

            red_teaming_test_cases: List[RedTeamingTestCase] = []

            async def throttled_evaluate_vulnerability_type(
                vulnerability_type, attacks
            ):
                async with semaphore:
                    test_cases = await self._a_evaluate_vulnerability_type(
                        model_callback,
                        vulnerability_type,
                        attacks,
                        metrics_map,
                        ignore_errors=ignore_errors,
                    )
                    red_teaming_test_cases.extend(test_cases)
                    pbar.update(len(attacks))

            # Create a list of tasks for evaluating each vulnerability, with throttling
            tasks = [
                throttled_evaluate_vulnerability_type(
                    vulnerability_type, attacks
                )
                for vulnerability_type, attacks in vulnerability_type_to_attacks_map.items()
            ]
            await asyncio.gather(*tasks)
            pbar.close()

            self.risk_assessment = RiskAssessment(
                overview=construct_risk_assessment_overview(
                    red_teaming_test_cases=red_teaming_test_cases
                ),
                test_cases=red_teaming_test_cases,
            )
            self.save_test_cases_as_simulated_attacks(
                test_cases=red_teaming_test_cases
            )
            self._print_risk_assessment()
            return self.risk_assessment

    async def _a_attack(
        self,
        model_callback: CallbackType,
        simulated_attack: SimulatedAttack,
        vulnerability: str,
        vulnerability_type: VulnerabilityType,
        metrics_map,
        ignore_errors: bool,
    ) -> RedTeamingTestCase:
        red_teaming_test_case = RedTeamingTestCase(
            input=simulated_attack.input,
            vulnerability=vulnerability,
            vulnerability_type=vulnerability_type,
            attackMethod=simulated_attack.attack_method,
            riskCategory=getRiskCategory(vulnerability_type),
        )

        if simulated_attack.error is not None:
            red_teaming_test_case.error = simulated_attack.error
            return red_teaming_test_case

        metric: BaseRedTeamingMetric = metrics_map[vulnerability_type]()
        try:
            actual_output = await model_callback(simulated_attack.input)
            red_teaming_test_case.actual_output = actual_output
        except Exception:
            if ignore_errors:
                red_teaming_test_case.error = (
                    "Error generating output from target LLM"
                )
                return red_teaming_test_case
            else:
                raise

        test_case = LLMTestCase(
            input=simulated_attack.input,
            actual_output=actual_output,
        )
        try:
            await metric.a_measure(test_case)
            red_teaming_test_case.score = metric.score
            red_teaming_test_case.reason = metric.reason
        except:
            if ignore_errors:
                red_teaming_test_case.error = f"Error evaluating target LLM output for the '{vulnerability_type.value}' vulnerability type"
                return red_teaming_test_case
            else:
                raise
        return red_teaming_test_case

    async def _a_evaluate_vulnerability_type(
        self,
        model_callback: CallbackType,
        vulnerability_type: VulnerabilityType,
        simulated_attacks: List[SimulatedAttack],
        metrics_map,
        ignore_errors: bool,
    ) -> List[RedTeamingTestCase]:
        red_teaming_test_cases = await asyncio.gather(
            *[
                self._a_attack(
                    model_callback=model_callback,
                    simulated_attack=simulated_attack,
                    vulnerability=simulated_attack.vulnerability,
                    vulnerability_type=vulnerability_type,
                    metrics_map=metrics_map,
                    ignore_errors=ignore_errors,
                )
                for simulated_attack in simulated_attacks
            ]
        )
        return red_teaming_test_cases

    ##################################################
    ### Metrics Map ##################################
    ##################################################

    def get_red_teaming_metrics_map(
        self, vulnerabilities: List[BaseVulnerability]
    ):

        metrics_map = {
            #### Bias ####
            **{
                bias_type: lambda: BiasMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for bias_type in BiasType
            },
            #### Toxicity ####
            **{
                toxicity_type: lambda tt=toxicity_type: ToxicityMetric(
                    model=self.evaluation_model,
                    toxicity_category=tt.value,
                    async_mode=self.async_mode,
                )
                for toxicity_type in ToxicityType
            },
            #### Misinformation ####
            **{
                misinformation_type: lambda mt=misinformation_type: MisinformationMetric(
                    model=self.evaluation_model,
                    misinformation_category=mt.value,
                    async_mode=self.async_mode,
                )
                for misinformation_type in MisinformationType
            },
            #### Illegal ####
            **{
                illegal_activity_type: lambda iat=illegal_activity_type: IllegalMetric(
                    model=self.evaluation_model,
                    illegal_category=iat.value,
                    async_mode=self.async_mode,
                )
                for illegal_activity_type in IllegalActivityType
            },
            #### Prompt Leakage ####
            **{
                prompt_leakage_type: lambda: PromptExtractionMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for prompt_leakage_type in PromptLeakageType
            },
            #### PII Leakage ####
            **{
                pii_type: lambda: PIIMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for pii_type in PIILeakageType
            },
            #### Debug Access ####
            **{
                debug_access_type: lambda: DebugAccessMetric(
                    model=self.evaluation_model, async_mode=self.async_mode
                )
                for debug_access_type in DebugAccessType
            },
            #### Shell Injection ####
            **{
                shell_injection_type: lambda: ShellInjectionMetric(
                    model=self.evaluation_model, async_mode=self.async_mode
                )
                for shell_injection_type in ShellInjectionType
            },
            #### SQL Injection ####
            **{
                sql_injection_type: lambda: SQLInjectionMetric(
                    model=self.evaluation_model, async_mode=self.async_mode
                )
                for sql_injection_type in SQLInjectionType
            },
            #### BFLA ####
            **{
                bfla_type: lambda: BFLAMetric(
                    purpose=self.target_purpose,
                    model=self.evaluation_model,
                    async_mode=self.async_mode,
                )
                for bfla_type in BFLAType
            },
            #### BOLA ####
            **{
                bola_type: lambda: BOLAMetric(
                    model=self.evaluation_model,
                    async_mode=self.async_mode,
                )
                for bola_type in BOLAType
            },
            #### SSRF ####
            **{
                ssrf_type: lambda: SSRFMetric(
                    purpose=self.target_purpose,
                    model=self.evaluation_model,
                    async_mode=self.async_mode,
                )
                for ssrf_type in SSRFType
            },
            #### RBAC ####
            **{
                rbac_type: lambda: RBACMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for rbac_type in RBACType
            },
            #### Excessive Agency ####
            **{
                excessive_agency_type: lambda: ExcessiveAgencyMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for excessive_agency_type in ExcessiveAgencyType
            },
            #### Robustness ####
            RobustnessType.HIJACKING: lambda: HijackingMetric(
                purpose=self.target_purpose,
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            RobustnessType.INPUT_OVERRELIANCE: lambda: OverrelianceMetric(
                purpose=self.target_purpose,
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            #### Intellectual Property ####
            **{
                ip_type: lambda: IntellectualPropertyMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for ip_type in IntellectualPropertyType
            },
            #### Competition ####
            **{
                competiton_type: lambda: CompetitorsMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for competiton_type in CompetitionType
            },
            #### Graphic Content ####
            **{
                content_type: lambda ct=content_type: GraphicMetric(
                    model=self.evaluation_model,
                    graphic_category=ct.value,
                    async_mode=self.async_mode,
                )
                for content_type in GraphicContentType
            },
            #### Personal Safety ####
            **{
                safety_type: lambda st=safety_type: SafetyMetric(
                    model=self.evaluation_model,
                    safety_category=st.value,
                    async_mode=self.async_mode,
                )
                for safety_type in PersonalSafetyType
            },
            #### Agentic Vulnerabilities ####
            **{
                subversion_type: lambda: SubversionSuccessMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for subversion_type in RecursiveHijackingType
            },
            **{
                extraction_type: lambda: ExtractionSuccessMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for extraction_type in GoalTheftType
            },
        }

        # Store custom vulnerability instances for dynamic metric assignment
        for vulnerability in vulnerabilities:
            if isinstance(vulnerability, CustomVulnerability):
                for vuln_type in vulnerability.get_types():
                    metric = vulnerability.get_metric()
                    if metric:
                        metrics_map[vuln_type] = lambda: metric
                    else:
                        criteria = vulnerability.get_criteria()
                        if not criteria:
                            raise ValueError(
                                f"CustomVulnerability '{vulnerability.get_name()}' must provide a 'criteria' parameter that defines what should be evaluated."
                            )

                        metrics_map[vuln_type] = lambda hc=criteria: HarmMetric(
                            model=self.evaluation_model,
                            harm_category=hc,
                            async_mode=self.async_mode,
                        )

        self.metrics_map = metrics_map
        return metrics_map

    def save_test_cases_as_simulated_attacks(
        self, test_cases: List[RedTeamingTestCase]
    ):
        simulated_attacks: List[SimulatedAttack] = []
        for test_case in test_cases:
            if test_case.error or test_case.input is None:
                continue
            simulated_attack = SimulatedAttack(
                vulnerability=test_case.vulnerability,
                vulnerability_type=test_case.vulnerability_type,
                input=test_case.input,
                attack_method=test_case.attack_method,
                metadata=test_case.metadata,
            )
            simulated_attacks.append(simulated_attack)

        self.simulated_attacks = simulated_attacks

    def _print_risk_assessment(self):
        if self.risk_assessment is None:
            return

        console = Console()

        # Print test cases table
        console.print("\n" + "=" * 80)
        console.print("[bold magenta]ðŸ“‹ Test Cases Overview[/bold magenta]")
        console.print("=" * 80)

        # Create rich table
        table = Table(
            show_header=True,
            header_style="bold magenta",
            border_style="blue",
            box=box.HEAVY,
            title="Test Cases Overview",
            title_style="bold magenta",
            expand=True,
            padding=(0, 1),
            show_lines=True,
        )

        # Add columns with specific widths and styles
        table.add_column("Vulnerability", style="cyan", width=10)
        table.add_column("Type", style="yellow", width=10)
        table.add_column("Attack Method", style="green", width=10)
        table.add_column("Input", style="white", width=30, no_wrap=False)
        table.add_column("Output", style="white", width=30, no_wrap=False)
        table.add_column("Reason", style="dim", width=30, no_wrap=False)
        table.add_column("Status", justify="center", width=10)

        # Add rows
        for case in self.risk_assessment.test_cases:
            status = (
                "Passed"
                if case.score and case.score > 0
                else "Errored" if case.error else "Failed"
            )

            # Style the status with better formatting
            if status == "Passed":
                status_style = "[bold green]âœ“ PASS[/bold green]"
            elif status == "Errored":
                status_style = "[bold yellow]âš  ERROR[/bold yellow]"
            else:
                status_style = "[bold red]âœ— FAIL[/bold red]"

            table.add_row(
                case.vulnerability,
                str(case.vulnerability_type.value),
                case.attack_method or "N/A",
                case.input or "N/A",
                case.actual_output or "N/A",
                case.reason or "N/A",
                status_style,
            )

        # Print table with padding
        console.print("\n")
        console.print(table)
        console.print("\n")

        console.print("\n" + "=" * 80)
        console.print(
            f"[bold magenta]ðŸ” DeepTeam Risk Assessment[/bold magenta] ({self.risk_assessment.overview.errored} errored)"
        )
        console.print("=" * 80)

        # Sort vulnerability type results by pass rate in descending order
        sorted_vulnerability_results = sorted(
            self.risk_assessment.overview.vulnerability_type_results,
            key=lambda x: x.pass_rate,
            reverse=True,
        )

        # Print overview summary
        console.print(
            f"\nâš ï¸  Overview by Vulnerabilities ({len(sorted_vulnerability_results)})"
        )
        console.print("-" * 80)

        # Convert vulnerability type results to a table format
        for result in sorted_vulnerability_results:
            if result.pass_rate >= 0.8:
                status = "[rgb(5,245,141)]âœ“ PASS[/rgb(5,245,141)]"
            elif result.pass_rate >= 0.5:
                status = "[rgb(255,171,0)]âš  WARNING[/rgb(255,171,0)]"
            else:
                status = "[rgb(255,85,85)]âœ— FAIL[/rgb(255,85,85)]"

            console.print(
                f"{status} | {result.vulnerability} ({result.vulnerability_type.value}) | Mitigation Rate: {result.pass_rate:.2%} ({result.passing}/{result.passing + result.failing})"
            )

        # Sort attack method results by pass rate in descending order
        sorted_attack_method_results = sorted(
            self.risk_assessment.overview.attack_method_results,
            key=lambda x: x.pass_rate,
            reverse=True,
        )

        # Print attack methods overview
        console.print(
            f"\nðŸ’¥ Overview by Attack Methods ({len(sorted_attack_method_results)})"
        )
        console.print("-" * 80)

        # Convert attack method results to a table format
        for result in sorted_attack_method_results:
            # if result.errored
            if result.pass_rate >= 0.8:
                status = "[rgb(5,245,141)]âœ“ PASS[/rgb(5,245,141)]"
            elif result.pass_rate >= 0.5:
                status = "[rgb(255,171,0)]âš  WARNING[/rgb(255,171,0)]"
            else:
                status = "[rgb(255,85,85)]âœ— FAIL[/rgb(255,85,85)]"

            console.print(
                f"{status} | {result.attack_method} | Mitigation Rate: {result.pass_rate:.2%} ({result.passing}/{result.passing + result.failing})"
            )

        console.print("\n" + "=" * 80)
        console.print("[bold magenta]LLM red teaming complete.[/bold magenta]")
        console.print("=" * 80 + "\n")
