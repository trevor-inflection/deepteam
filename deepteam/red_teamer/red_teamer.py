import asyncio
from tqdm import tqdm
from typing import Dict, List, Optional, Union
from pydantic import BaseModel
import inspect


from deepeval.models import DeepEvalBaseLLM
from deepeval.metrics.utils import initialize_model
from deepeval.dataset.golden import Golden
from deepeval.test_case import LLMTestCase
from deepeval.utils import get_or_create_event_loop
from deepeval.telemetry import capture_red_teamer_run

from deepteam.attacks import BaseAttack
from deepteam.vulnerabilities import BaseVulnerability
from deepteam.vulnerabilities.types import (
    IntellectualPropertyType,
    UnauthorizedAccessType,
    IllegalActivityType,
    ExcessiveAgencyType,
    PersonalSafetyType,
    GraphicContentType,
    MisinformationType,
    PromptLeakageType,
    CompetitionType,
    PIILeakageType,
    RobustnessType,
    ToxicityType,
    BiasType,
    VulnerabilityType,
)
from deepteam.attacks.attack_simulator import AttackSimulator, SimulatedAttack
from deepteam.attacks.multi_turn.types import CallbackType
from deepteam.metrics import (
    BaseRedTeamingMetric,
    BiasMetric,
    HarmMetric,
    PromptExtractionMetric,
    PIIMetric,
    RBACMetric,
    DebugAccessMetric,
    ShellInjectionMetric,
    SQLInjectionMetric,
    BFLAMetric,
    BOLAMetric,
    SSRFMetric,
    ExcessiveAgencyMetric,
    HijackingMetric,
    IntellectualPropertyMetric,
    OverrelianceMetric,
    CompetitorsMetric,
)
from deepteam.red_teamer.utils import group_attacks_by_vulnerability_type
from deepteam.red_teamer.risk_assessment import (
    construct_risk_assessment_overview,
    RedTeamingTestCase,
    RiskAssessment,
)
from deepteam.risks import getRiskCategory


class RedTeamer:
    risk_assessment: Optional[RiskAssessment] = None
    simulated_attacks: Optional[List[SimulatedAttack]] = None

    def __init__(
        self,
        simulator_model: Optional[
            Union[str, DeepEvalBaseLLM]
        ] = "gpt-3.5-turbo-0125",
        evaluation_model: Optional[Union[str, DeepEvalBaseLLM]] = "gpt-4o",
        async_mode: bool = True,
        max_concurrent: int = 10,
    ):
        self.target_purpose = ""
        self.simulator_model, _ = initialize_model(simulator_model)
        self.evaluation_model, _ = initialize_model(evaluation_model)
        self.async_mode = async_mode
        self.synthetic_goldens: List[Golden] = []
        self.max_concurrent = max_concurrent
        self.attack_simulator = AttackSimulator(
            simulator_model=self.simulator_model,
            purpose=self.target_purpose,
            max_concurrent=max_concurrent,
        )

    def red_team(
        self,
        model_callback: CallbackType,
        vulnerabilities: List[BaseVulnerability],
        attacks: List[BaseAttack],
        attacks_per_vulnerability_type: int = 1,
        ignore_errors: bool = False,
        reuse_simulated_attacks: bool = False,
    ):
        if self.async_mode:
            assert inspect.iscoroutinefunction(
                model_callback
            ), "`model_callback` needs to be async. `async_mode` has been set to True."
            loop = get_or_create_event_loop()
            return loop.run_until_complete(
                self.a_red_team(
                    model_callback=model_callback,
                    attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                    vulnerabilities=vulnerabilities,
                    attacks=attacks,
                    ignore_errors=ignore_errors,
                    reuse_simulated_attacks=reuse_simulated_attacks,
                )
            )
        else:
            assert not inspect.iscoroutinefunction(
                model_callback
            ), "`model_callback` needs to be sync. `async_mode` has been set to False."
            with capture_red_teamer_run(
                attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                vulnerabilities=vulnerabilities,
                attack_enhancements=attacks,
            ):
                # Initialize metric map
                metrics_map = self.get_red_teaming_metrics_map()

                # Simulate attacks
                if (
                    reuse_simulated_attacks
                    and self.simulated_attacks is not None
                    and len(self.simulated_attacks) > 0
                ):
                    simulated_attacks: List[SimulatedAttack] = (
                        self.simulated_attacks
                    )
                else:
                    simulated_attacks: List[SimulatedAttack] = (
                        self.attack_simulator.simulate(
                            attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                            vulnerabilities=vulnerabilities,
                            attacks=attacks,
                            ignore_errors=ignore_errors,
                        )
                    )

                vulnerability_type_to_attacks_map = (
                    group_attacks_by_vulnerability_type(simulated_attacks)
                )
                red_teaming_test_cases: List[RedTeamingTestCase] = []
                total_vulnerability_types = sum(
                    len(v.get_types()) for v in vulnerabilities
                )
                pbar = tqdm(
                    vulnerability_type_to_attacks_map.items(),
                    desc=f"ðŸ“ Evaluating {total_vulnerability_types} vulnerability types across {len(vulnerabilities)} vulnerability(s)",
                )
                for vulnerability_type, simulated_attacks in pbar:
                    for simulated_attack in simulated_attacks:
                        metric: BaseRedTeamingMetric = metrics_map.get(
                            vulnerability_type
                        )()

                        red_teaming_test_case = RedTeamingTestCase(
                            vulnerability=simulated_attack.vulnerability,
                            vulnerability_type=vulnerability_type.value,
                            attackMethod=simulated_attack.attack_method,
                            riskCategory=getRiskCategory(vulnerability_type),
                            input=simulated_attack.input,
                        )

                        # this will only go through if ignore_errors == True
                        if simulated_attack.error:
                            red_teaming_test_case.error = simulated_attack.error
                            red_teaming_test_cases.append(red_teaming_test_case)
                            continue

                        try:
                            target_output = model_callback(
                                simulated_attack.input
                            )
                            red_teaming_test_case.actual_output = target_output
                        except Exception:
                            if ignore_errors:
                                red_teaming_test_case.error = (
                                    "Error generating output from target LLM"
                                )
                                red_teaming_test_cases.append(
                                    red_teaming_test_case
                                )
                                continue
                            else:
                                raise

                        test_case = LLMTestCase(
                            input=simulated_attack.input,
                            actual_output=target_output,
                        )
                        try:
                            metric.measure(test_case)
                            red_teaming_test_case.score = metric.score
                            red_teaming_test_case.reason = metric.reason
                        except Exception:
                            if ignore_errors:
                                red_teaming_test_case.error = f"Error evaluating target LLM output for the '{vulnerability_type.value}' vulnerability"
                                red_teaming_test_cases.append(
                                    red_teaming_test_case
                                )
                                continue
                            else:
                                raise

                        red_teaming_test_cases.append(red_teaming_test_case)

                self.risk_assessment = RiskAssessment(
                    overview=construct_risk_assessment_overview(
                        test_cases=red_teaming_test_cases
                    ),
                    test_cases=red_teaming_test_cases,
                )

                self.save_test_cases_as_simulated_attacks(
                    test_cases=red_teaming_test_cases
                )
                return self.risk_assessment

    async def a_red_team(
        self,
        model_callback: CallbackType,
        vulnerabilities: List[BaseVulnerability],
        attacks: List[BaseAttack],
        attacks_per_vulnerability_type: int = 1,
        ignore_errors: bool = False,
        reuse_simulated_attacks: bool = False,
    ):
        with capture_red_teamer_run(
            attacks_per_vulnerability_type=attacks_per_vulnerability_type,
            vulnerabilities=vulnerabilities,
            attack_enhancements=attacks,
        ):
            # Initialize metric map
            metrics_map = self.get_red_teaming_metrics_map()

            # Generate attacks
            if (
                reuse_simulated_attacks
                and self.simulated_attacks is not None
                and len(self.simulated_attacks) > 0
            ):
                simulated_attacks: List[SimulatedAttack] = (
                    self.simulated_attacks
                )
            else:
                simulated_attacks: List[SimulatedAttack] = (
                    await self.attack_simulator.a_simulate(
                        attacks_per_vulnerability_type=attacks_per_vulnerability_type,
                        vulnerabilities=vulnerabilities,
                        attacks=attacks,
                        ignore_errors=ignore_errors,
                    )
                )

            # Create a mapping of vulnerabilities to attacks
            vulnerability_type_to_attacks_map: Dict[
                VulnerabilityType, List[SimulatedAttack]
            ] = {}
            for simulated_attack in simulated_attacks:
                if (
                    simulated_attack.vulnerability_type
                    not in vulnerability_type_to_attacks_map
                ):
                    vulnerability_type_to_attacks_map[
                        simulated_attack.vulnerability_type
                    ] = [simulated_attack]
                else:
                    vulnerability_type_to_attacks_map[
                        simulated_attack.vulnerability_type
                    ].append(simulated_attack)

            semaphore = asyncio.Semaphore(self.max_concurrent)
            total_attacks = sum(
                len(attacks)
                for attacks in vulnerability_type_to_attacks_map.values()
            )
            num_vulnerability_types = sum(
                len(v.get_types()) for v in vulnerabilities
            )
            pbar = tqdm(
                total=total_attacks,
                desc=f"ðŸ“ Evaluating {num_vulnerability_types} vulnerability types across {len(vulnerabilities)} vulnerability(s)",
            )

            red_teaming_test_cases: List[RedTeamingTestCase] = []

            async def throttled_evaluate_vulnerability_type(
                vulnerability_type, attacks
            ):
                async with semaphore:
                    test_cases = await self._a_evaluate_vulnerability_type(
                        model_callback,
                        vulnerability_type,
                        attacks,
                        metrics_map,
                        ignore_errors=ignore_errors,
                    )
                    red_teaming_test_cases.extend(test_cases)
                    pbar.update(len(attacks))

            # Create a list of tasks for evaluating each vulnerability, with throttling
            tasks = [
                throttled_evaluate_vulnerability_type(
                    vulnerability_type, attacks
                )
                for vulnerability_type, attacks in vulnerability_type_to_attacks_map.items()
            ]
            await asyncio.gather(*tasks)
            pbar.close()

            self.risk_assessment = RiskAssessment(
                overview=construct_risk_assessment_overview(
                    test_cases=red_teaming_test_cases
                ),
                test_cases=red_teaming_test_cases,
            )
            self.save_test_cases_as_simulated_attacks(
                test_cases=red_teaming_test_cases
            )
            return self.risk_assessment

    async def _a_attack(
        self,
        model_callback: CallbackType,
        simulated_attack: SimulatedAttack,
        vulnerability: str,
        vulnerability_type: VulnerabilityType,
        metrics_map,
        ignore_errors: bool,
    ) -> RedTeamingTestCase:
        red_teaming_test_case = RedTeamingTestCase(
            input=simulated_attack.input,
            vulnerability=vulnerability,
            vulnerability_type=vulnerability_type,
            attack_method=simulated_attack.attack_method,
            riskCategory=getRiskCategory(vulnerability_type),
        )

        if simulated_attack.error is not None:
            red_teaming_test_case.error = simulated_attack.error
            return red_teaming_test_case

        metric: BaseRedTeamingMetric = metrics_map[vulnerability_type]()
        try:
            actual_output = await model_callback(simulated_attack.input)
            red_teaming_test_case.actual_output = actual_output
        except Exception:
            if ignore_errors:
                red_teaming_test_case.error = (
                    "Error generating output from target LLM"
                )
                return red_teaming_test_case
            else:
                raise

        test_case = LLMTestCase(
            input=simulated_attack.input,
            actual_output=actual_output,
        )
        try:
            await metric.a_measure(test_case)
            red_teaming_test_case.score = metric.score
            red_teaming_test_case.reason = metric.reason
        except:
            if ignore_errors:
                red_teaming_test_case.error = f"Error evaluating target LLM output for the '{vulnerability_type.value}' vulnerability type"
                return red_teaming_test_case
            else:
                raise
        return red_teaming_test_case

    async def _a_evaluate_vulnerability_type(
        self,
        model_callback: CallbackType,
        vulnerability_type: VulnerabilityType,
        simulated_attacks: List[SimulatedAttack],
        metrics_map,
        ignore_errors: bool,
    ) -> List[RedTeamingTestCase]:
        red_teaming_test_cases = await asyncio.gather(
            *[
                self._a_attack(
                    model_callback=model_callback,
                    simulated_attack=simulated_attack,
                    vulnerability=simulated_attack.vulnerability,
                    vulnerability_type=vulnerability_type,
                    metrics_map=metrics_map,
                    ignore_errors=ignore_errors,
                )
                for simulated_attack in simulated_attacks
            ]
        )
        return red_teaming_test_cases

    ##################################################
    ### Metrics Map ##################################
    ##################################################

    def get_red_teaming_metrics_map(self):
        metrics_map = {
            #### Bias ####
            **{
                bias_type: lambda: BiasMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for bias_type in BiasType
            },
            #### Toxicity ####
            **{
                toxicity_type: lambda tt=toxicity_type: HarmMetric(
                    model=self.evaluation_model,
                    harm_category=tt.value,
                    async_mode=self.async_mode,
                )
                for toxicity_type in ToxicityType
            },
            #### Misinformation ####
            **{
                misinformation_type: lambda mt=misinformation_type: HarmMetric(
                    model=self.evaluation_model,
                    harm_category=mt.value,
                    async_mode=self.async_mode,
                )
                for misinformation_type in MisinformationType
            },
            #### Illegal ####
            **{
                illegal_activity_type: lambda iat=illegal_activity_type: HarmMetric(
                    model=self.evaluation_model,
                    harm_category=iat.value,
                    async_mode=self.async_mode,
                )
                for illegal_activity_type in IllegalActivityType
            },
            #### Prompt Leakage ####
            **{
                prompt_leakage_type: lambda: PromptExtractionMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for prompt_leakage_type in PromptLeakageType
            },
            #### PII Leakage ####
            **{
                pii_type: lambda: PIIMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for pii_type in PIILeakageType
            },
            #### Unauthorized Access ####
            UnauthorizedAccessType.DEBUG_ACCESS: lambda: DebugAccessMetric(
                model=self.evaluation_model, async_mode=self.async_mode
            ),
            UnauthorizedAccessType.RBAC: lambda: RBACMetric(
                model=self.evaluation_model,
                purpose=self.target_purpose,
                async_mode=self.async_mode,
            ),
            UnauthorizedAccessType.SHELL_INJECTION: lambda: ShellInjectionMetric(
                model=self.evaluation_model, async_mode=self.async_mode
            ),
            UnauthorizedAccessType.SQL_INJECTION: lambda: SQLInjectionMetric(
                model=self.evaluation_model, async_mode=self.async_mode
            ),
            UnauthorizedAccessType.BFLA: lambda: BFLAMetric(
                purpose=self.target_purpose,
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            UnauthorizedAccessType.BOLA: lambda: BOLAMetric(
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            UnauthorizedAccessType.SSRF: lambda: SSRFMetric(
                purpose=self.target_purpose,
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            #### Excessive Agency ####
            **{
                excessive_agency_type: lambda: ExcessiveAgencyMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for excessive_agency_type in ExcessiveAgencyType
            },
            #### Robustness ####
            RobustnessType.HIJACKING: lambda: HijackingMetric(
                purpose=self.target_purpose,
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            RobustnessType.INPUT_OVERRELIANCE: lambda: OverrelianceMetric(
                purpose=self.target_purpose,
                model=self.evaluation_model,
                async_mode=self.async_mode,
            ),
            #### Intellectual Property ####
            **{
                ip_type: lambda: IntellectualPropertyMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for ip_type in IntellectualPropertyType
            },
            #### Competition ####
            **{
                competiton_type: lambda: CompetitorsMetric(
                    model=self.evaluation_model,
                    purpose=self.target_purpose,
                    async_mode=self.async_mode,
                )
                for competiton_type in CompetitionType
            },
            #### Graphic Content ####
            **{
                content_type: lambda ct=content_type: HarmMetric(
                    model=self.evaluation_model,
                    harm_category=ct.value,
                    async_mode=self.async_mode,
                )
                for content_type in GraphicContentType
            },
            #### Personal Safety ####
            **{
                safety_type: lambda st=safety_type: HarmMetric(
                    model=self.evaluation_model,
                    harm_category=st.value,
                    async_mode=self.async_mode,
                )
                for safety_type in PersonalSafetyType
            },
        }
        self.metrics_map = metrics_map
        return metrics_map

    def save_test_cases_as_simulated_attacks(
        self, test_cases: List[RedTeamingTestCase]
    ):
        simulated_attacks: List[SimulatedAttack] = []
        for test_case in test_cases:
            if test_case.error or test_case.input is None:
                continue
            simulated_attack = SimulatedAttack(
                vulnerability=test_case.vulnerability,
                vulnerability_type=test_case.vulnerability_type,
                input=test_case.input,
                attack_method=test_case.attack_method,
            )
            simulated_attacks.append(simulated_attack)

        self.simulated_attacks = simulated_attacks
